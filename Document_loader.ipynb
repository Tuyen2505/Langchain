{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/pdf/2312.16862\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(url, extract_images=True)\n",
    "docs = pdf_loader.load()\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 0}, page_content='TinyGPT-V: Efficient Multimodal Large Language Model\\nvia Small Backbones\\nZhengqing Yuan1 Zhaoxu Li 2 Weiran Huang3 Yanfang Ye1 Lichao Sun 4\\nAbstract\\nIn recent years, multimodal large language mod-\\nels (MLLMs) such as GPT-4V have demonstrated\\nremarkable advancements, excelling in a variety\\nof vision-language tasks. Despite their prowess,\\nthe closed-source nature and computational de-\\nmands of such models limit their accessibility and\\napplicability. This study introduces TinyGPT-V ,\\na novel open-source MLLM, designed for effi-\\ncient training and inference across various vision-\\nlanguage tasks, including image captioning (IC)\\nand visual question answering (VQA). Leverag-\\ning a compact yet powerful architecture, TinyGPT-\\nV integrates the Phi-2 language model with pre-\\ntrained vision encoders, utilizing a unique map-\\nping module for visual and linguistic information\\nfusion. With a training regimen optimized for\\nsmall backbones and employing a diverse dataset\\namalgam, TinyGPT-V requires significantly lower\\ncomputational resources—24GB for training and\\nas little as 8GB for inference—without compro-\\nmising on performance. Our experiments demon-\\nstrate that TinyGPT-V , with its language model\\n2.8 billion parameters, achieves comparable re-\\nsults in VQA and image inference tasks to its\\nlarger counterparts while being uniquely suited\\nfor deployment on resource-constrained devices\\nthrough innovative quantization techniques. This\\nwork not only paves the way for more accessible\\nand efficient MLLMs but also underscores the po-\\ntential of smaller, optimized models in bridging\\nthe gap between high performance and computa-\\ntional efficiency in real-world applications. Addi-\\ntionally, this paper introduces a new approach to\\nmultimodal large language models using smaller\\nbackbones. Our code and training weights are\\navailable in the supplementary material.\\n1Universiy of Notre Dame 2Nanyang Technological University\\n3Shanghai Jiao Tong University 4Lehigh University. Correspon-\\ndence to: Lichao Sun <lis221@lehigh.edu>.\\nAccepted to the Workshop on Advancing Neural Network Training\\nat International Conference on Machine Learning (W ANT@ICML\\n2024).\\n5.07.510.012.515.0\\nParameter (Billions)\\n38\\n40\\n42\\n44\\n46Performance Flamingo\\nBLIP-2\\nLLaVA\\nInstructBLIP\\nMiniGPT-4\\nTinyGPT-V\\nFigure 1: Comparison of TinyGPT-V with other current\\nMLLMs models shows TinyGPT-V achieves cost-effective,\\nefficient, and high-performing with fewer parameters.\\n1. Introduction\\nIn recent years, the field of artificial intelligence has seen\\nsignificant advancements through the development of multi-\\nmodal large language models (MLLMs), such as GPT-4V ,\\nwhich have shown exceptional performance across a range\\nof vision-language tasks (Yang et al., 2023). Despite GPT-\\n4V’s impressive capabilities, its closed-source nature limits\\nits widespread application and adaptability. In contrast, the\\nopen-source landscape for MLLMs is rapidly evolving, pre-\\nsenting models like LLaV A and MiniGPT-4 that excel in\\nimage captioning (IC), visual question answering (VQA)\\noften comparable GPT-4V in these areas (Dai et al., 2023;\\nLiu et al., 2023a;b; Zhu et al., 2023). Notably, MiniGPT-\\nv2 (Chen et al., 2023) has demonstrated superior perfor-\\nmance in various visual grounding and question-answering\\ntasks. However, its training code remains proprietary, which\\nposes challenges for community-driven advancements and\\nadaptability.\\nAlthough the impressive vision-language capabilities\\ndemonstrated by some open-source MLLMs, they fre-\\nquently necessitate significant computational resources for\\ntraining and inference. For example, training LLaV A-v1.5-\\n13B (Liu et al., 2023a) required 8 × A100 GPUs, each\\nequipped with 80GB of memory, cumulating in 25.5 hours\\nof continuous training. As shown in Figure 2 (a), the un-\\n1\\narXiv:2312.16862v3  [cs.CV]  21 Jun 2024')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 0}, page_content='TinyGPT-V: Efficient Multimodal Large Language Model\\nvia Small Backbones\\nZhengqing Yuan1 Zhaoxu Li 2 Weiran Huang3 Yanfang Ye1 Lichao Sun 4\\nAbstract\\nIn recent years, multimodal large language mod-\\nels (MLLMs) such as GPT-4V have demonstrated\\nremarkable advancements, excelling in a variety\\nof vision-language tasks. Despite their prowess,\\nthe closed-source nature and computational de-\\nmands of such models limit their accessibility and\\napplicability. This study introduces TinyGPT-V ,\\na novel open-source MLLM, designed for effi-\\ncient training and inference across various vision-\\nlanguage tasks, including image captioning (IC)\\nand visual question answering (VQA). Leverag-\\ning a compact yet powerful architecture, TinyGPT-\\nV integrates the Phi-2 language model with pre-\\ntrained vision encoders, utilizing a unique map-\\nping module for visual and linguistic information\\nfusion. With a training regimen optimized for\\nsmall backbones and employing a diverse dataset\\namalgam, TinyGPT-V requires significantly lower\\ncomputational resources—24GB for training and\\nas little as 8GB for inference—without compro-\\nmising on performance. Our experiments demon-\\nstrate that TinyGPT-V , with its language model\\n2.8 billion parameters, achieves comparable re-\\nsults in VQA and image inference tasks to its\\nlarger counterparts while being uniquely suited\\nfor deployment on resource-constrained devices\\nthrough innovative quantization techniques. This\\nwork not only paves the way for more accessible\\nand efficient MLLMs but also underscores the po-\\ntential of smaller, optimized models in bridging\\nthe gap between high performance and computa-\\ntional efficiency in real-world applications. Addi-\\ntionally, this paper introduces a new approach to\\nmultimodal large language models using smaller\\nbackbones. Our code and training weights are\\navailable in the supplementary material.\\n1Universiy of Notre Dame 2Nanyang Technological University\\n3Shanghai Jiao Tong University 4Lehigh University. Correspon-\\ndence to: Lichao Sun <lis221@lehigh.edu>.\\nAccepted to the Workshop on Advancing Neural Network Training\\nat International Conference on Machine Learning (W ANT@ICML\\n2024).\\n5.07.510.012.515.0\\nParameter (Billions)\\n38\\n40\\n42\\n44\\n46Performance Flamingo\\nBLIP-2\\nLLaVA\\nInstructBLIP\\nMiniGPT-4\\nTinyGPT-V\\nFigure 1: Comparison of TinyGPT-V with other current\\nMLLMs models shows TinyGPT-V achieves cost-effective,\\nefficient, and high-performing with fewer parameters.\\n1. Introduction\\nIn recent years, the field of artificial intelligence has seen\\nsignificant advancements through the development of multi-\\nmodal large language models (MLLMs), such as GPT-4V ,\\nwhich have shown exceptional performance across a range\\nof vision-language tasks (Yang et al., 2023). Despite GPT-\\n4V’s impressive capabilities, its closed-source nature limits\\nits widespread application and adaptability. In contrast, the\\nopen-source landscape for MLLMs is rapidly evolving, pre-\\nsenting models like LLaV A and MiniGPT-4 that excel in\\nimage captioning (IC), visual question answering (VQA)\\noften comparable GPT-4V in these areas (Dai et al., 2023;\\nLiu et al., 2023a;b; Zhu et al., 2023). Notably, MiniGPT-\\nv2 (Chen et al., 2023) has demonstrated superior perfor-\\nmance in various visual grounding and question-answering\\ntasks. However, its training code remains proprietary, which\\nposes challenges for community-driven advancements and\\nadaptability.\\nAlthough the impressive vision-language capabilities\\ndemonstrated by some open-source MLLMs, they fre-\\nquently necessitate significant computational resources for\\ntraining and inference. For example, training LLaV A-v1.5-\\n13B (Liu et al., 2023a) required 8 × A100 GPUs, each\\nequipped with 80GB of memory, cumulating in 25.5 hours\\nof continuous training. As shown in Figure 2 (a), the un-\\n1\\narXiv:2312.16862v3  [cs.CV]  21 Jun 2024'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 1}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nProjection (110M)\\n0.8%\\nEVA-VIT (0.98B)\\n7.0%\\nVicuna (13B)\\n92.3%\\nProjection (119M)\\n3.1%\\nEVA-VIT (0.98B)\\n25.1%\\nPhi-2 (2.8B)\\n71.8%\\n(a) MiniGPT-4 (a) TinyGPT-V\\nFigure 2: (a) is the occupancy ratio of each component in MiniGPT-4, and (b) is the occupancy ratio of each component in\\nTinyGPT-V . We have considerably narrowed down the occupancy ratio of the Language model in MLLMs.\\nderlying performance of large language models, which are\\nintegral to MLLMs, is pivotal. Models such as LLaV A-\\nv1.5-13B and MiniGPT-v2 depend on high-capacity back-\\nbones which are Vicuna-13b-v1.5 (Zheng et al., 2023) and\\nLLaMA2-7B-Chat (Touvron et al., 2023), respectively, ne-\\ncessitating a substantial number of parameters to effectively\\ntackle complex tasks including IC and VQA.\\nWe introduce TinyGPT-V , a novel model designed for effi-\\ncient training and inference, requiring only 24GB of GPU\\nmemory for training and as little as 8GB of GPU or CPU\\nmemory for inference. This model makes use of the ad-\\nvanced large language model Phi-2 (Javaheripi et al., 2023)\\nand incorporates pre-trained vision modules from (Li et al.,\\n2023a) and CLIP (Radford et al., 2021) as its vision encoder,\\ncoupled with a mapping module to facilitate the integration\\nof visual information. During training, TinyGPT-V adopts\\na novel training methodology focused on small pre-trained\\nbackbones, unlike any other MLLMs, utilizing the unique\\nmapping module between the visual encoder and the lan-\\nguage model as well as novelty normalization methods,\\nwhile keeping all other components frozen. For its train-\\ning dataset, TinyGPT-V employs the multi-tasks datasets,\\nincluding LAION (Schuhmann et al., 2021), Conceptual\\nCaptions (Changpinyo et al., 2021; Sharma et al., 2018),\\nSBU (Ordonez et al., 2011), and others (Lin et al., 2015;\\nSchwenk et al., 2022; Hudson & Manning, 2019; Kiela et al.,\\n2020; Lu et al., 2021; Gurari et al., 2018; Mao et al., 2016;\\nKazemzadeh et al., 2014; Yu et al., 2016).\\nIn our study, we found that TinyGPT-V exhibits similar traits\\nwith GPT-4, especially when doing some VQA and image\\ninference. With only 2.8 billion parameters of its language\\nmodel, TinyGPT-V employs a unique quantization process,\\nlike using 8-bit quantization, making it well-suited for local\\ndeployment and inference on 8GB mobile devices. This\\nmodel represents a significant advancement in achieving a\\nbalance between exceptional performance and efficiency in\\nMLLMs, as shown in Figure 1. Our work not only aims\\nto enable the community to develop more cost-effective,\\nefficient, and high-performing MLLMs for widespread real-\\nworld applications but also introduces a training framework\\noptimized for small pre-trained backbones.\\n2. Related Work\\nAdvanced language model. The evolution of language\\nmodels has been marked by significant milestones, start-\\ning with early successes like GPT2 (Radford et al., 2019)\\nand BERT (Devlin et al., 2018) in natural language process-\\ning (NLP). These foundational models set the stage for the\\nsubsequent development of vastly larger language models,\\nencompassing hundreds of billions of parameters. This dra-\\nmatic increase in scale has led to the emergence of advanced\\ncapabilities as seen in models like GPT-3 (Brown et al.,\\n2020), Chinchilla (Hoffmann et al., 2022), OPT (Zhang\\net al., 2022), and BLOOM (Workshop et al., 2022). These\\nlarge language models (LLMs) have been instrumental\\nin further advancements in the field. For instance, Chat-\\nGPT (OpenAI, 2022) and InstructGPT (Ouyang et al., 2022)\\nleverage these powerful models to answer diverse ques-\\ntions and perform complex tasks such as coding. The in-\\ntroduction of open-source LLMs like LLaMA (Touvron\\net al., 2023) has further propelled research in this area, in-\\nspiring subsequent developments like Alpaca (Taori et al.,\\n2023), Vicuna (Chiang et al., 2023). These models fine-tune\\nthe LLaMA model with additional high-quality instruction\\ndatasets, showcasing the versatility and adaptability of LLM\\nframeworks.Among the most notable recent advancements\\nare Phi (Li et al., 2023b) and its successor, Phi-2 (Javaheripi\\net al., 2023). These models have demonstrated exceptional\\nperformance, rivaling or even surpassing models up to 25\\ntimes larger in scale. This indicates a significant shift in\\n2'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 2}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nFigure 3: Compared to other general-purpose MLLMs, our TinyGPT-V achieves the same performance as 13B or 7B models\\nin a variety of visual language tasks.\\nthe landscape of language modeling, emphasizing efficiency\\nand effectiveness without necessarily relying on sheer size.\\nMultimodal language model. In recent years, the trend of\\naligning visual input to large language models for vision-\\nlanguage tasks has gained significant attention (Chen et al.,\\n2022; Tsimpoukelli et al., 2021; Alayrac et al., 2022; Li\\net al., 2023a; Liu et al., 2023b;a; Zhu et al., 2023; Chen\\net al., 2023). Seminal works like VisualGPT (Chen et al.,\\n2022) and Frozen (Tsimpoukelli et al., 2021), which uti-\\nlized pre-trained language models for image captioning and\\nvisual question answering. This approach was further ad-\\nvanced by models such as Flamingo (Alayrac et al., 2022),\\nwhich incorporated gated cross-attention mechanisms to\\nalign pre-trained vision encoders and language models, train-\\ning on vast image-text pairs. BLIP-2 (Li et al., 2023a)\\nintroduced an efficient Q-Former for aligning visual and\\nlanguage modalities. These groundbreaking studies have\\npaved the way for further innovations in the field, leading to\\nthe development of models like LLaV A (Liu et al., 2023b)\\nand MiniGPT4 (Zhu et al., 2023), and their subsequent iter-\\nations, LLaV A-v1.5 (Liu et al., 2023a), MiniGPT-v2 (Chen\\net al., 2023), ArtGPT-4 (Yuan et al., 2023), instruction GPT-\\n4 (Wei et al., 2023) and Instruction Mining (Cao et al.,\\n2023). These models have demonstrated advanced multi-\\nmodal capabilities through instruction tuning, showcasing\\nremarkable generalization abilities.\\n3. Method\\nWe briefly introduce our vision-language model, TinyGPT-\\nV , followed by an analysis of its structure, culminating in a\\ndetailed description of the training process for each stage.\\n3.1. Model Architecture\\nIn this subsection, we present the architecture of TinyGPT-V ,\\nwhich consists of a visual encoder, projection layers, and a\\nlarge language model, as shown in Figure 4.\\nVisual encoder backbone. In the TinyGPT-V , it utilizes\\nEV A (Fang et al., 2022) of the ViT serves as the visual\\nfoundation model, which remains inactive during the entire\\ntraining process. Our model operates at an image resolution\\nof 224x224 for Stages 1, 2, and 3, and at 448x448 for Stage\\n4. The positional encoding is enhanced to accommodate the\\nincreased image resolution which is known as the Relative\\nPosition Bias (Dufter et al., 2021). It enhances the model’s\\nunderstanding of the spatial relationships between elements\\nin an image.\\nProjection layers. The Projection layers embed visual\\nfeatures extracted by the visual encoder into the language\\nmodel, enhancing the model’s ability to process image-\\nbased information. We adopt the Q-Former layers from\\nthe BLIP-2 architecture (Li et al., 2023a) as the initial pro-\\n3IconVQ\\nIconVQ\\nIconvQ\\nVSR\\nVSR\\nVSR\\n100\\n100\\n100\\n80\\n80\\n80\\n60\\n60\\n60\\n40\\n40\\n40\\nGQA\\nGQA\\nGQA\\nVizWiz\\nVizWiz\\nVizWiz\\nFlamingo\\nBLIP-2\\nHM\\nHM\\nHM\\nLLaVA\\nIconvQ\\nInstructBLIP\\nIconVQ\\nIconvQ\\nMiniGPT-4\\nTinyGPT-V\\nVSR\\nVSR\\nVSR\\n100\\n100\\n100\\n8Q\\n80\\n80\\n60\\n60\\n60\\n40\\n40\\n40\\nGQA\\nGQA\\nGQA\\nVizWiz\\nVizWiz\\nVizWiz\\nHM\\nHM\\nHM'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 3}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nPhi-2\\nLoRA+Normalization\\nFreeze Train\\nQ-Former\\nViT\\nLinear 1 (MiniGPT-4)\\nLinear 2\\nThis is an image of an alpaca. \\nAlpacas are domesticated \\nspecies of South American \\ncamelids, known for their \\nsoft fluffy coats which are \\nused for making wool.\\n[vqa]What’s this?\\nFigure 4: Architecture of TinyGPT-V . The model takes a\\nvisual backbone, which remains frozen during all training\\nphases. We concatenate Q-Former layer visual output tokens\\nfrom ViT backbone and project them into Phi-2 language\\nmodel space via two linear projection layers.\\njection layer, aiming to leverage the full potential of the\\npre-trained BLIP system within visual language models.\\nThis strategy significantly reduces the number of parameters\\nneeding training. The second and third layers are linear\\nprojection layers, designed to bridge the dimensionality gap\\nbetween the Q-Former output and the language model’s\\nembedding layer, thereby aligning visual tokens more effec-\\ntively with the language model’s hidden space. As shown in\\nFigure 6, to expedite TinyGPT-V’s training, we initially use\\na pre-trained Linear Projection from MiniGPT-4 (Vicuna\\n7B) as the second layer. We then introduce an additional lin-\\near projection layer, initialized with a Gaussian distribution,\\nas the third layer to seamlessly integrate into the hidden\\nspace of the Phi-2 model.\\nLarge lanuguage model backbone. Our TinyGPT-V large\\nlanguage model is built upon the Phi-2 model (Javaheripi\\net al., 2023) as its backbone. Phi-2, a 2.7 billion-parameter\\nlanguage model, exhibits exceptional reasoning and lan-\\nguage comprehension abilities, achieving state-of-the-art\\nperformance among language models with fewer than 13\\nbillion parameters. In complex benchmarks, Phi-2 either\\nmatches or exceeds the performance of models up to 25\\ntimes its size. We primarily use Phi-2’s linguistic abilities\\nto do various vision-language tasks. Specifically, for vision\\nreasoning tasks that involve spatial location identification,\\nwe instruct the linguistic model to generate textual descrip-\\ntions of what will happen in the next scenario, representing\\ntheir objects’ coordinates, as shown in Table 8.\\nNormalization and LoRA for TinyGPT-V .In Section 4.4,\\nwe conclude that training smaller-scale large language mod-\\nels for transfer learning, especially across different modali-\\nties (e.g., from text to image), poses significant challenges.\\nOur studies indicate that these smaller models are prone to\\nencountering NaN or INF values during multimodal data\\ncomputations. This issue often leads to a computational\\nloss value of NaN, thereby causing failure in the initial\\nbatch forward propagation. Moreover, the limited number\\nof trainable parameters in these models may lead to gradient\\nvanishing during training. To mitigate these problems, as\\ndepicted in Figure 5 (c), we incorporate the post-norm and\\ninput norm mechanisms from LLaMA-2, applying RMS\\nNorm after each Multi-Head Attention Layer (MHA) to\\nnormalize data for downstream processing. In addition, we\\nhave to update all layer norms in the Phi-2 model to improve\\ntraining stability, as detailed in the subsequent equation.\\nLayerNorminput(xhidden) =γ xhidden − µ√\\nσ2 + ϵ\\n+ β (1)\\nWhere, xhidden is the input of this layer, µ and σ2 are the\\nmean and variance of the inputs to the layer, respectively,ϵ\\nis a small number to prevent division by zero, γ and β are\\ntrainable parameters.\\nRMSNorm(xpost) = xpostq\\n1\\nN\\nPN\\ni=1 x2\\ni + ϵ\\n(2)\\nwhere xpost is the input after MHA, N is the dimension of\\nxpost.\\nFurthermore, (Henry et al., 2020) have underscored the vital\\nrole of Query-Key Normalization in low-resource learning\\nscenarios. Hence, as show in Figure 5 (d), we have incor-\\nporated Query-Key Normalization into the Phi-2 model, as\\ndetailed in the following equation.\\nAttention(Q, K, V) =softmax\\n\\x12LayerNorm(Q)LayerNorm(K)T\\n√dk\\n\\x13\\nV\\n(3)\\nwhere dk denotes the dimension of Q or K.\\nThe structure of the LoRA mechanism (Hu et al., 2021)\\nis show in Figure 5 (a), which is an efficient fine-tuning\\nmethod in parallel to the frozen pre-training weights as\\nshown in Figure 5 (c), which does not increase the inference\\ntime consuming for large language models and is easier to\\noptimize.\\n3.2. Training Stages\\nIn this subsection, the four-stage training process of\\nTinyGPT-V will be described.\\nWarm-up training for the first training stage. During\\nthe initial pretraining stage, TinyGPT-V is taught vision-\\nlanguage understanding using large datasets of aligned\\n4'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 4}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nLinear\\nDown\\nr\\nLinear\\nUpPretrained\\nWeights\\n\\ud835\\nInput\\nOutput\\nLoRA\\n(a) LoRA\\nMLP\\nNormalization\\nMHA LoRA\\n(b) LoRA Module \\nfor LLMs Block\\nMHA\\nQuery-Key \\nNormalization\\nLayer Norm\\nRMS Norm\\nMLP\\nLoRA\\nAfter \\nStage1\\n(c) LLMs Block\\n for TinyGPT-V\\nAttention\\nQ K V\\nLayer \\nNorm\\nLayer \\nNorm\\n(d) Query-Key \\nNormalization for MHA\\nFreeze Train Data Pathway Conditional Pathway\\nFigure 5: (a) represents the structure of LoRA, (b) represents how LoRA can efficiently fine-tune large language models\\n(LLMs) in natural language processing, (c) represents the structure of LLMs for TinyGPT-V , and (d) represents the structure\\nof QK Normalization.\\nPhi-2\\nImage-text Pair \\nInstruction Learning\\nFreezeTrain\\nStage 1\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\nPhi-2\\nStage 2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nPhi-2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nStage 3\\nPhi-2\\nVisual Encode \\n& Q-Former\\nNormalization\\nMiniGPT-4 \\nProj.\\nLinear\\n& LoRA\\nMulti-Tasks Learning\\nStage 4\\nFigure 6: The training process of TinyGPT-V , the first stage\\nis warm-up training, the second stage is pre-training, the\\nthird stage is instruction fine-tuning, and the fourth stage is\\nmulti-task learning.\\nimage-text pairs. The model identifies the output from the\\nprojection layers as a soft prompt directing it to create rel-\\nevant texts and to allow large language models to accept\\ninputs from the image modality. The pretraining process\\nuses a dataset combination of Conceptual Caption, SBU,\\nand LAION, involving 20000 training steps covering about\\n5 million image-text pairs.\\nPre-training for the second training stage. Following the\\ninitial training stage, the large language model becomes\\nequipped to process image modality inputs. To guarantee\\nmore consistent performance as the model transitions into\\nthe subsequent training stage, we re-employ the dataset from\\nthe first stage, specifically for training the LoRA module.\\nInstruction tuning for the third training stage. We fine-\\ntuned this TinyGPT-V model using a selection of image-text\\npairings from MiniGPT4 or LLaV A, which included in-\\nstructions like “###Human: <Img><ImageHere></Img>\\nTake a look at this image and describe what you no-\\ntice.###Assistant:.”. We used a uniform template inclusive\\nof a randomly chosen prompt that improved the model’s\\ncapacity for generating responses that were consistent and\\nsounded more natural.\\nMulti-task learning in the fourth training stage. The\\nfourth training stage of TinyGPT-V focuses on enhancing\\nits conversation ability as a chatbot by tuning the model with\\nmore multi-modal instruction datasets as shown in Table 1,\\nincluding LLaV A, Flickr30k, a mixing multi-task dataset,\\nand Unnatural Instruction using multi-tasks template as de-\\ntailed in appendix A. The LLaV A dataset is utilized for\\nmulti-modal instruction tuning with detailed descriptions\\nand complex reasoning examples. The Flickr30k dataset\\nis used to improve grounded image caption generation and\\nobject parsing and grounding capabilities. Additionally, a\\nmixing multi-task dataset is created to improve the model’s\\nhandling of multiple tasks during multi-round conversations.\\nFinally, to recover the language generation ability, the Un-\\nnatural Instruction dataset is added to the third-stage training\\nof TinyGPT-V .\\n5'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 5}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nData types Dataset Stage 1 Stage 2 Stage 3 Stage 4\\nImage-text pair LAION, CC3M, SBU ✓ ✓ ✗ ✗\\nInstruction tuning MiniGPT-4 Stage2 for CC & SBU ✗ ✗ ✓ ✗\\nCaption Text Captions, COCO Captions ✗ ✗ ✗ ✓\\nREC RefCOCO, RefCOCO+, RefCOCOg, Visual Genome ✗ ✗ ✗ ✓\\nVQA GQA, VQAv2, OK-VQA, AOK-VQA, OCR-VQA ✗ ✗ ✗ ✓\\nMultimodal instruction LLaV A dataset, Flickr30k, Multi-task conversation ✗ ✗ ✗ ✓\\nLangauge dataset Unnatural Instructions ✗ ✗ ✗ ✓\\nTable 1: The full list of datasets used by TinyGPT-MoE during training.\\nFigure 7: Changes in loss during the training stage of TinyGPT-V .\\n4. Experiments\\nIn this section, we describe the training and evaluation meth-\\nods in detail.\\n4.1. Training\\nExperimental setting. The experimental environment for\\nthis study was established with a single NVIDIA RTX 3090\\nGPU, equipped with a substantial 24GB of VRAM. The cen-\\ntral processing was handled by an AMD EPYC 7552 48-core\\nProcessor, offering 15 virtual CPUs. Memory allocation was\\nset at 80GB, ensuring sufficient capacity for handling large\\ndatasets. The software environment was standardized on\\nPyTorch version 2.0.0, with CUDA 11.8 support, facilitating\\noptimized tensor operations on the GPU.\\nTraining process. In our experimental process, we meticu-\\nlously orchestrated the training of our model through four\\ndistinct stages, each characterized by specific learning rate\\nstrategies and loss profiles, as shown in Figure 7.\\nStage 1: Spanning 17 epochs, with each epoch consisting\\nof 1000 iterations, we employed a dynamic learning rate\\napproach. The learning rate commenced at 1e-5 at the begin-\\nning of each epoch and gradually ascended to 1e-4 by the\\nepoch’s end. This pattern was consistently applied across all\\n17 epochs. The training loss exhibited a steady decline, start-\\ning from 7.152 and progressively tapering down to 2.620,\\nreflecting the model’s increasing proficiency in learning\\nfrom the data. The purpose of this stage is to be able to\\nmake the Phi-2 model in TinyGPT-V react in some way to\\nthe input of the imaging modality. The alignment of text\\nand image in the semantic space is done.\\nStage 2: Comprising 4 epochs, each with 5000 iterations,\\nthis stage introduced the “linear_warmup_cosine_lr“ (He\\net al., 2018; Goyal et al., 2018) learning rate schedule. We\\ninitiated a warmup phase of 5000 steps, where the learn-\\ning rate linearly increased from 1e-6 (warmup_lr) to 1e-4\\n(init_lr), followed by a cosine decay down to a minimum\\nlearning rate of 8e-5. This phase saw a consistent reduction\\nin loss, starting at 2.726 and culminating at 2.343. The\\npurpose of this stage is to enable the LoRA module to play\\na role in multimodal data, further reducing the model’s loss\\non image-text pairs and improving the model’s ability to\\nlearn from the data.\\nStage 3: This stage lasted for 5 epochs, each with 200\\niterations. We maintained the “linear_warmup_cosine_lr“\\nschedule, with a warmup phase of 200 steps. The learning\\nrate began at 1e-6, ascending to 3e-5 (init_lr), before decay-\\ning to 1e-5 (min_lr). The loss values reflected significant\\n6876５\\nWarm-up\\nPretrain\\n+InstructionTuning\\nSS\\n4\\nMulti-Tasks Learning\\nLoS\\nTraining\\n3\\n2\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\nEpoch'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 6}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nimprovements, starting at 1.992 and reducing to 1.125. The\\npurpose of this stage is to allow TinyGPT-V to accept both\\nverbal and image modal inputs and produce responses to\\nthem. After this stage of training TinyGPT-V has been able\\nto perform most of the image answering tasks.\\nStage 4: The final stage stretched over 50 epochs, each\\ncomprising 1000 iterations. We adhered to the “lin-\\near_warmup_cosine_lr“ schedule with a 1000-step warmup\\nphase. The learning rate was initiated at 1e-6, reaching up\\nto 1e-5 (init_lr), and then experiencing a cosine decay to\\na minimum of 8e-5. The training loss values displayed a\\nconsistent downward trajectory, beginning at 2.720 and ulti-\\nmately reaching as low as 1.399. The purpose of this stage is\\nto allow TinyGPT-V to perform various tasks such as VQA\\nor VSR tasks at the same time, increasing the generalization\\nperformance of TinyGPT-V on multimodal tasks.\\n4.2. Evaluation\\nEvaluation datasets. GQA (Hudson & Manning, 2019)\\nis a dataset for real-world visual reasoning and composi-\\ntional question answering, featuring a powerful question en-\\ngine that generates 22 million diverse reasoning questions.\\nVSR (Liu et al., 2023b) comprises over 10k natural text-\\nimage pairs in English, encompassing 66 types of spatial\\nrelations. IconQA (Lu et al., 2021) with 107,439 questions\\naimed at challenging visual understanding and reasoning in\\nthe context of icon images, encompassing three sub-tasks\\n(multi-image-choice, multi-text-choice, and filling-in-the-\\nblank). VizWiz (Gurari et al., 2018) is a collection of more\\nthan 31,000 visual queries, each derived from a photo taken\\nby a visually impaired individual using a smartphone, ac-\\ncompanied by a vocalized question regarding the image,\\nand supplemented with 10 answers sourced from a crowd\\nfor each query. The Hateful Memes dataset (HM) (Kiela\\net al., 2021), developed by Facebook AI, is a comprehensive\\nmultimodal collection specifically designed for the detec-\\ntion of hateful content in memes, combining both image\\nand text elements, and comprises over 10,000 newly created\\nmultimodal examples.\\nVisual question answering results. As shown in Table 2,\\nit becomes evident that TinyGPT-V , a language model with\\nonly 2.8 billion parameters, exhibits notably competitive per-\\nformance across multiple benchmarks, closely rivaling mod-\\nels with nearly 13 billion parameters. Specifically, in the\\nVSR (Visual-Spatial Reasoning) zero-shot task, TinyGPT-V\\noutshines its counterparts by securing the highest score of\\n54.7%. This is particularly impressive considering its pa-\\nrameter size is approximately 4.6 times smaller than other\\nleading models such as BLIP-2, LLaV A, and InstructBLIP.\\nIn the GQA benchmark, while TinyGPT-V scores are 38.9%,\\nit lags behind the highest score achieved by InstructBLIP,\\nwhich is 49.5%. However, TinyGPT-V shows robust perfor-\\nTinyGPT-V and others answer example compare\\nUres [vqa] where should I hide in this room when playing hide and\\nseek\\nLLaV A-1.5hide behind the bookshelf\\nMiniGPT-v2behind couch\\nGPT-4V Behind the Couch\\nUnder the Table\\nInside the Bookshelf\\nBehind the Curtains\\nBehind the TV\\nTinyGPT-Vunder couch\\nFigure 8: Comparison of reasoning answers from different\\nModels. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\nmance in the IconVQ challenge, attaining a score of 44.7%,\\njust 0.1% short of InstructBLIP’s leading score of 44.8%.\\nSimilarly, in the VizWiz task, TinyGPT-V demonstrates\\ncommendable capabilities with a score of 37.8%, which,\\nis not only the highest but is notable given its reduced pa-\\nrameter count. In the context of the Hateful Memes (HM)\\ndataset, TinyGPT-V matches InstructBLIP’s top score of\\n57.5% with its own score of 54.0%, again underscoring its\\nefficiency and capacity to compete with models of larger\\nscales. Overall, TinyGPT-V’s performance across these di-\\nverse and challenging benchmarks is striking, especially\\nwhen considering its parameter efficiency\\n4.3. Qualitative Evaluation\\nThe comparative analysis revealed TinyGPT-V’s distinct\\nadvantage in delivering concise and accurate visual interpre-\\ntations. In the reasoning task to find a hiding spot during\\na game of hide and seek, TinyGPT-V demonstrated its su-\\nperior capability by providing a singular, viable suggestion:\\n’under couch’. This contrasts with other models that either\\noffered multiple options, some of which were incorrect as\\nindicated by the text in red (e.g., GPT-4V suggesting ’In-\\nside the Bookshelf’), or specified less practical hiding spots.\\nWhen asked about potential activities in an image with an\\nalligator, TinyGPT-V suggested a cautious response with-\\nout speculating beyond what was visible. In contrast, other\\nmodels, like LLaV A-1.5, provided extended narratives that\\nintroduced assumptions not directly inferred from the image.\\nSimilarly, in describing a soccer match scene, TinyGPT-V’s\\nresponse was succinct and focused on the key elements,\\navoiding the inaccuracies noted in MiniGPT-v2’s account,\\nwhich incorrectly identified multiple soccer balls on the\\n7'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 7}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nMethod LLM GQA VSR IconVQ VizWiz HM AverageParameters (zero-shot) (zero-shot) (zero-shot) (zero-shot)\\nFlamingo 9B - 31.8 - 28.8 57.0 39.20\\nIDEFICS (Laurençon et al., 2023) 7B - 38.4 - 35.5 - 37.05\\n65B - 45.2 - 36.0 - 39.60\\nBLIP-2 13B 41.0 50.9 40.6 19.6 53.7 41.16\\nLLaV A 13B 41.3 51.2 43.0 - - 45.17\\nInstructBLIP 13B 49.5 52.1 44.8 33.4 57.5 47.45\\nMiniGPT-4 13B - - 35.9 - - 35.90\\nBLIV A (Hu et al., 2023) 7B - - 44.8 31.4 55.6 41.15\\nLLaV A-Phi (Zhu et al., 2024) 2.8B - - 54.1 37.6 - 43.15\\nMoE-LLaV A (Lin et al., 2024)∗ 1.8B×4 61.5 - - 32.6 - 47.50\\nOurs\\nTinyGPT-V (Phi-2) 2.8B 38.9 54.7 44.7 37.8 54.0 46.02\\nTinyGPT-V (Phi-1.5) 1.3B 34.3 35.8 37.2 28.4 50.3 37.2\\nTable 2: Comparative performance of TinyGPT-V and other MLLMs across multiple visual question answering benchmarks.\\n∗It is worth noting that MoE-LLaV A is required 8xA100-80G for training.\\nMethod TinyGPT-V LLaV A MiniGPT-4\\nseconds per words 0.067 0.426 0.300\\ninference occupancy (8-bit) 5.6GB 22GB 23.5GB\\nTable 3: Comparison of inference time and inference occu-\\npancy about devices.\\npitch. These examples, as tabulated in Table 5 and Table 7,\\nillustrate TinyGPT-V’s superior performance in generating\\nbrief yet precise responses, underscoring its practicality for\\nrapid and reliable visual question answering. For efficient\\nevaluation, as shown in table 3, TinyGPT-V operates at\\nthe fastest pace, taking only 0.067 seconds to generate a\\nword, which suggests upper efficiency in processing speed\\ncompared to LLaV A and MiniGPT-4. On the other hand,\\nLLaV A exhibits a significantly slower word generation time\\nat 0.426 seconds per word, coupled with a higher memory\\noccupancy of 22GB. MiniGPT-4, with a generation time of\\n0.300 seconds per word and a memory usage of 23.5GB.\\n4.4. Ablation Study\\nAs shown in Table 4, the full TinyGPT-V model achieves\\nlow loss across all stages, but the removal of key modules\\nleads to significant training issues. Without the LoRA mod-\\nule, there’s a gradient vanish starting from Stage 3. Omitting\\nInput Layer Norm increases loss notably (to 2.839 in Stage\\n1) and causes gradient vanishing in Stage 4. Without RMS\\nNorm, the model sees an elevated loss in Stage 1 (2.747)\\nand faces early gradient vanishing in Stage 2. The absence\\nof QK Norm results in immediate gradient vanish. This data\\nclearly illustrates each module’s crucial role in preventing\\ngradient vanishing and maintaining low loss throughout the\\ntraining process.\\nMethod Stage 1 Loss Stage 2 Loss Stage 3 Loss Stage 4 LossTinyGPT-V 2.620 2.343 1.125 1.330w/o LoRA 2.620 - Gradient Vanish -w/o Input Layer Norm 2.839 2.555 1.344 Gradient Vanishw/o RMS Norm 2.747 Gradient Vanish - -w/o QK Norm Gradient Vanish - - -\\nTable 4: Importance of each module in TinyGPT-V at each\\nstage of training.\\nFurthermore, our reveal a notable trend: the smaller the\\nlarge language model used for transfer learning (particu-\\nlarly in transitioning from text-to-image modality), the more\\nchallenging the training process becomes. We observed a\\npronounced need for additional normalization layers to sta-\\nbilize the training, especially when scaling down from larger\\nmodels like Vicuna-13B to smaller ones like Phi-2 (2.7B),\\nPhi-1.5 (1.3B), and other small backbones as detailed in the\\nAppendix B.\\n5. Conclusion\\nIn this study, we introduce TinyGPT-V , a parameter-efficient\\nMLLMs tailored for a range of real-world vision-language\\napplications. Our model innovatively builds on the compact\\nyet powerful Phi-2 small language model framework. This\\napproach results in TinyGPT-V delivering exceptional out-\\ncomes in diverse benchmarks like visual question-answering\\nand referring expression comprehension while keeping com-\\nputational demands manageable. Remarkably, TinyGPT-V\\ncan be trained on a 24G GPU and deployed on an 8G device,\\ndemonstrating a significant advancement in creating cost-\\neffective, efficient, and potent MLLMs. This paper marks a\\ncontribution towards crafting smaller, yet robust multimodal\\nlanguage models for practical, real-world use cases. We\\nenvision that our work will catalyze further explorations\\ninto developing compact MLLMs for diverse applications.\\n8'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 8}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nReferences\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\\nHasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., et al. Flamingo: a visual language model for few-shot\\nlearning. Advances in Neural Information Processing\\nSystems, 35:23716–23736, 2022.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:\\n1877–1901, 2020.\\nCao, Y ., Kang, Y ., Wang, C., and Sun, L. Instruction mining:\\nWhen data mining meets large language model finetuning,\\n2023.\\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\\nceptual 12m: Pushing web-scale image-text pre-training\\nto recognize long-tail visual concepts, 2021.\\nChen, J., Guo, H., Yi, K., Li, B., and Elhoseiny, M. Visual-\\ngpt: Data-efficient adaptation of pretrained language mod-\\nels for image captioning. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npp. 18030–18040, 2022.\\nChen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krish-\\nnamoorthi, R., Chandra, V ., Xiong, Y ., and Elhoseiny, M.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478, 2023.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y ., Gonzalez, J. E.,\\net al. Vicuna: An open-source chatbot impressing gpt-4\\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\\n(accessed 14 April 2023), 2023.\\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\\nW., Li, B., Fung, P., and Hoi, S. Instructblip: Towards\\ngeneral-purpose vision-language models with instruction\\ntuning, 2023.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805,\\n2018.\\nDufter, P., Schmitt, M., and Schütze, H. Position informa-\\ntion in transformers: An overview, 2021.\\nFang, Y ., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X.,\\nHuang, T., Wang, X., and Cao, Y . Eva: Exploring the\\nlimits of masked visual representation learning at scale,\\n2022.\\nGoyal, P., Dollár, P., Girshick, R., Noordhuis, P.,\\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and\\nHe, K. Accurate, large minibatch sgd: Training imagenet\\nin 1 hour, 2018.\\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\\nAnswering visual questions from blind people. In Pro-\\nceedings of the IEEE conference on computer vision and\\npattern recognition, pp. 3608–3617, 2018.\\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\\nBag of tricks for image classification with convolutional\\nneural networks, 2018.\\nHenry, A., Dachapally, P. R., Pawar, S., and Chen, Y . Query-\\nkey normalization for transformers, 2020.\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., et al. Training compute-optimal\\nlarge language models. arXiv preprint arXiv:2203.15556,\\n2022.\\nHu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,\\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation\\nof large language models, 2021.\\nHu, W., Xu, Y ., Li, Y ., Li, W., Chen, Z., and Tu, Z. Bliva:\\nA simple multimodal llm for better handling of text-rich\\nvisual questions, 2023.\\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\\nreal-world visual reasoning and compositional question\\nanswering. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pp. 6700–\\n6709, 2019.\\nJavaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck,\\nS., Mendes, C. C. T., Chen, W., Giorno, A. D., Eldan,\\nR., Gopi, S., Gunasekar, S., Javaheripi, M., Kauff-\\nmann, P., Lee, Y . T., Li, Y ., Nguyen, A., de Rosa, G.,\\nSaarikivi, O., Salim, A., Shah, S., Santacroce, M.,\\nBehl, H. S., Kalai, A. T., Wang, X., Ward, R., Witte,\\nP., Zhang, C., and Zhang, Y . Phi-2: The surprising\\npower of small language models. https://www.\\nmicrosoft.com/en-us/research/blog/\\nphi-2-the-surprising-power-of-small-language-models/ ,\\n2023.\\nKazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T.\\nReferitgame: Referring to objects in photographs of natu-\\nral scenes. In Proceedings of the 2014 conference on em-\\npirical methods in natural language processing (EMNLP),\\npp. 787–798, 2014.\\n9'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 9}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes.\\nAdvances in neural information processing systems, 33:\\n2611–2624, 2020.\\nKiela, D., Firooz, H., Mohan, A., Goswami, V ., Singh, A.,\\nRingshia, P., and Testuggine, D. The hateful memes\\nchallenge: Detecting hate speech in multimodal memes,\\n2021.\\nLaurençon, H., Saulnier, L., Tronchon, L., Bekman, S.,\\nSingh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush,\\nA. M., Kiela, D., Cord, M., and Sanh, V . Obelics: An\\nopen web-scale filtered dataset of interleaved image-text\\ndocuments, 2023.\\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping\\nlanguage-image pre-training with frozen image encoders\\nand large language models, 2023a.\\nLi, Y ., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar,\\nS., and Lee, Y . T. Textbooks are all you need ii: phi-1.5\\ntechnical report, 2023b.\\nLin, B., Tang, Z., Ye, Y ., Cui, J., Zhu, B., Jin, P., Huang, J.,\\nZhang, J., Ning, M., and Yuan, L. Moe-llava: Mixture of\\nexperts for large vision-language models, 2024.\\nLin, T.-Y ., Maire, M., Belongie, S., Bourdev, L., Girshick,\\nR., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and\\nDollár, P. Microsoft coco: Common objects in context,\\n2015.\\nLiu, H., Li, C., Li, Y ., and Lee, Y . J. Improved baselines\\nwith visual instruction tuning, 2023a.\\nLiu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction\\ntuning. arXiv preprint arXiv:2304.08485, 2023b.\\nLu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y ., Zhang, W., Yu,\\nZ., Liang, X., and Zhu, S.-C. Iconqa: A new benchmark\\nfor abstract diagram understanding and visual language\\nreasoning. arXiv preprint arXiv:2110.13214, 2021.\\nMao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L.,\\nand Murphy, K. Generation and comprehension of unam-\\nbiguous object descriptions. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 11–20, 2016.\\nOpenAI. Introducing chatgpt. https://openai.com/\\nblog/chatgpt, 2022.\\nOrdonez, V ., Kulkarni, G., and Berg, T. Im2text: Describing\\nimages using 1 million captioned photographs. In\\nShawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and\\nWeinberger, K. (eds.), Advances in Neural Information\\nProcessing Systems, volume 24. Curran Associates, Inc.,\\n2011. URL https://proceedings.neurips.\\ncc/paper_files/paper/2011/file/\\n5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.\\npdf.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\nwith human feedback. Advances in Neural Information\\nProcessing Systems, 35:27730–27744, 2022.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\\nSutskever, I., et al. Language models are unsupervised\\nmultitask learners. OpenAI blog, 1(8):9, 2019.\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\\nJ., Krueger, G., and Sutskever, I. Learning transferable\\nvisual models from natural language supervision, 2021.\\nSchuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,\\nR., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and\\nKomatsuzaki, A. Laion-400m: Open dataset of clip-\\nfiltered 400 million image-text pairs, 2021.\\nSchwenk, D., Khandelwal, A., Clark, C., Marino, K., and\\nMottaghi, R. A-okvqa: A benchmark for visual ques-\\ntion answering using world knowledge. In European\\nConference on Computer Vision, pp. 146–162. Springer,\\n2022.\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning. In Gurevych,\\nI. and Miyao, Y . (eds.), Proceedings of the 56th An-\\nnual Meeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers), pp. 2556–2565, Mel-\\nbourne, Australia, July 2018. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/P18-1238. URL\\nhttps://aclanthology.org/P18-1238.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X.,\\nGuestrin, C., Liang, P., and Hashimoto, T. B. Stanford\\nalpaca: An instruction-following llama model, 2023.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V ., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV ., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y .,\\nMao, Y ., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y ., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\n10'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 10}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023.\\nTsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S.,\\nVinyals, O., and Hill, F. Multimodal few-shot learn-\\ning with frozen language models. Advances in Neural\\nInformation Processing Systems, 34:200–212, 2021.\\nWei, L., Jiang, Z., Huang, W., and Sun, L. Instructiongpt-\\n4: A 200-instruction paradigm for fine-tuning minigpt-4,\\n2023.\\nWorkshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E.,\\nIli´c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon,\\nF., et al. Bloom: A 176b-parameter open-access multilin-\\ngual language model. arXiv preprint arXiv:2211.05100,\\n2022.\\nYang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and\\nWang, L. The dawn of lmms: Preliminary explorations\\nwith gpt-4v(ision), 2023.\\nYu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L.\\nModeling context in referring expressions. In Computer\\nVision–ECCV 2016: 14th European Conference, Amster-\\ndam, The Netherlands, October 11-14, 2016, Proceed-\\nings, Part II 14, pp. 69–85. Springer, 2016.\\nYuan, Z., Wang, X., Wang, K., Sun, L., and Ye, Y . Artgpt-\\n4: Towards artistic-understanding large vision-language\\nmodels with enhanced adapter, 2023.\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V .,\\net al. Opt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068, 2022.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z.,\\nZhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,\\nH., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge\\nwith mt-bench and chatbot arena, 2023.\\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\\nMinigpt-4: Enhancing vision-language understanding\\nwith advanced large language models. arXiv preprint\\narXiv:2304.10592, 2023.\\nZhu, Y ., Zhu, M., Liu, N., Ou, Z., Mou, X., and Tang,\\nJ. Llava-phi: Efficient multi-modal assistant with small\\nlanguage model, 2024.\\n11'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 11}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nA. Multi-task Instruction Template\\nTo reduce ambiguity in training a unified multimodal model for diverse tasks such as visual question answering, image\\ncaptioning, referring expression comprehension, generation, and object parsing and grounding, we employed task-specific\\ntokens from MiniGPT-v2 within a multitask instruction template. This template, derived from the LLaMA-2 conversation\\ntemplate (Touvron et al., 2023), includes a general input format comprising image features, a task identifier token, and an\\ninstruction input. We incorporated six distinct task identifiers, each associated with a particular task. For tasks necessitating\\nthe identification of spatial locations of referred objects, the model uses textual representations of bounding boxes with\\ncoordinates normalized between 0 and 100. Overall, MiniGPT-v2’s unique task-specific tokens enhance task disambiguation,\\nleading to more precise and effective task execution.\\nTinyGPT-V and others answer example compare\\nUres What might happen in this image in the next second\\nLLaV A-1.5 In the image, a man is sitting on a log next to a large alligator, and he is holding a camera. It is likely\\nthat the man is about to take a photograph or record a video of the alligator. The next second could\\ninvolve the man capturing the alligator’s behavior, appearance, or any other interesting moment.\\nThe alligator might also react to the man’s presence, such as moving or showing curiosity, which\\ncould make the captured image or video more engaging and memorable.\\nMiniGPT-v2 In the next second, the man might be startled or surprised by the close-up shot of the crocodile\\nin the water. This could cause him to jump back or react with caution, ensuring his safety in the\\npresence of the wild animal. The image captures a moment of wonder and curiosity, highlighting\\nthe beauty of the natural environment while also acknowledging the potential danger that comes\\nwith being so close to a wild animal.\\nGPT-4V In the next second, the man might quickly move away to maintain a safe distance from the alligator.\\nAlligators can be unpredictable, and it’s important to keep a safe distance from them at all times to\\navoid potential harm. The man seems to be taking a selfie with the alligator, which suggests that he\\nis close to the animal. This is generally not advisable as it can be dangerous. It’s always important\\nto respect wildlife and observe it from a distance that is safe for both the person and the animal.\\nTinyGPT-V The image shows a man holding a selfie camera next to a crocodile. This could be an unusual and\\nunexpected sight, as crocodiles are not typically associated with human activities like taking selfies.\\nlt’s important for the man to stay safe and avoid any potential risks posed by the crocodile.\\nTable 5: Comparison of prediction answers from different models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\nB. Small Backbones for Transfer Learning\\nAs shown in Table 6, a striking pattern emerges from the data: smaller LLMs exhibit heightened sensitivity to the removal\\nof these modules, with a pronounced tendency towards training difficulties, such as gradient vanishing. For instance, the\\nabsence of LoRA in both Phi-1.5 and TinyLLaMA resulted in an immediate cessation of training progress post-Stage 1,\\nindicating a critical reliance on this module for sustaining training in smaller models. Similarly, the exclusion of QK Norm\\nled to gradient vanishing at the earliest stage across all smaller LLMs, underscoring its essential role in the initial phases\\n12'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 12}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nof training. Moreover, the sequential progression in training losses across stages for models without these modifications\\ndemonstrates a clear degradation in training efficiency and effectiveness. For example, the removal of Input Layer Norm and\\nRMS Norm not only heightened Stage 1 loss across Phi-1.5 and TinyLLaMA but also precipitated gradient vanishing in\\nlater stages, showcasing the compound impact of these modules on model stability and learning capability. This analysis\\nincontrovertibly highlights a fundamental challenge in training smaller LLMs for migration to MLLMs: the absence of\\nkey architectural and normalization modules severely impedes their training process, making them more prone to early\\ntraining halts and efficiency losses. The results underscore the necessity of these components in supporting the stability and\\ngradual learning progression of smaller LLMs, thus illuminating a pivotal consideration for developers aiming to optimize\\nthe training framework for seamless model.\\nLLM Stage 1 Loss Stage 2 Loss Stage 3 Loss Stage 4 Loss\\nPhi-2 (2.7B) 2.620 2.343 1.125 1.330\\nPhi-1.5 (1.3B) 3.420 3.043 1.525 1.730\\nw/o LoRA 3.420 - Gradient Vanish -\\nw/o Input Layer Norm 3.555 3.221 1.544 Gradient Vanish\\nw/o RMS Norm 3.557 Gradient Vanish - -\\nw/o QK Norm Gradient Vanish - - -\\nTinyLLaMA (1.1B) 3.529 3.053 1.371 1.830\\nw/o LoRA 3.529 - Gradient Vanish -\\nw/o Input Layer Norm 3.611 3.331 1.444 Gradient Vanish\\nw/o RMS Norm 3.607 Gradient Vanish - -\\nw/o QK Norm Gradient Vanish - - -\\nTable 6: Importance of each module and different LLMs in TinyGPT-V training framework at each stage of training.\\n13'), Document(metadata={'source': 'https://arxiv.org/pdf/2312.16862', 'page': 13}, page_content='Submission and Formatting Instructions for the W ANT@ICML 2024\\nTinyGPT-V and others answer example compare\\nUres What’s in the image?\\nLLaV A-1.5 The image shows a group of men playing soccer on a field. They are actively engaged in the game,\\nwith some players running towards the soccer ball.\\nMiniGPT-v2 The image features a lively and competitive soccer game with several players on the field. Some of\\nthe players are wearing jerseys, while others have their own unique outfits. There are at least eight\\nplayers in total, with three of them being the main focus of the scene.\\nOne player is kicking a soccer ball, with another player nearby trying to block the shot. A third\\nplayer is also present, playing offense. In addition, there are two soccer balls visible on the field,\\none closer to the left side and the other near the center of the scene.\\nThe players are engaged in intense action, showcasing their skills and determination to score. The\\natmosphere appears to be a high-energy match with a mix of excitement and competition among\\nthe players.\\nGPT-4V The image shows a moment from a soccer match. In the foreground, there’s a player in a blue\\nuniform who appears to be dribbling the ball with close control, evading two opponents in white\\nuniforms. The player in blue is crouched low to the ground, demonstrating agility and focus, which\\nsuggests he is in the midst of making a strategic play, possibly attempting to bypass the defenders. In\\nthe background, other players can be observed, and the stands are filled with spectators, indicating\\nthat this is a professional match likely taking place in a large stadium. The attire and hairstyles\\nsuggest the photo may be from a past era, possibly the 1980s or 1990s.\\nTinyGPT-V The image shows a socer game with several people playing, including at least one person in a blue\\nshirt. The players are engaged in a friendly match, showcasing their skills and teamwork.\\nTable 7: Comparison of descriptive answers from different Models. Text in red indicates incorrect suggestions. The\\nTinyGPT-V’s answer was short and precise.\\n14')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Load dir pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "path_dir = \"./data_source\"\n",
    "pdf_loader = PyPDFDirectoryLoader(path_dir)\n",
    "docs = pdf_loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Load web html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebBaseLoader\n\u001b[0;32m      4\u001b[0m web_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "web_paths = \"#\"\n",
    "\n",
    "classes = ['title','content','header']\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_ = classes)\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths = web_paths,\n",
    "    bs_kwargs= dict(\n",
    "        parse_only = bs4_strainer\n",
    "    )\n",
    ")\n",
    "\n",
    "docs = web_loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
